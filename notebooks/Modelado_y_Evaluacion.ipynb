{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Modelado y Evaluaci√≥n de Modelos\n",
        "## Proyecto Final - MLOps con CRISP-DM\n",
        "\n",
        "**Fase 4: Modelado con MLflow**  \n",
        "**Fase 5: Evaluaci√≥n**\n",
        "\n",
        "Este notebook contiene:\n",
        "- Entrenamiento de 4 modelos diferentes (Logistic Regression, Random Forest, XGBoost, LightGBM)\n",
        "- Hyperparameter tuning para cada modelo\n",
        "- Comparaci√≥n sistem√°tica de modelos\n",
        "- Evaluaci√≥n completa con m√©tricas y visualizaciones\n",
        "- Registro de modelos en MLflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaci√≥n de Librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Librer√≠as importadas correctamente\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Agregar src al path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "import mlflow.lightgbm\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# M√©tricas\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, log_loss, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve, classification_report\n",
        ")\n",
        "\n",
        "# Configuraci√≥n\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuraci√≥n de MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar MLflow\n",
        "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
        "EXPERIMENT_NAME = \"churn-prediction-experiment\"\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "print(f\"‚úÖ MLflow configurado\")\n",
        "print(f\"üìä Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
        "print(f\"üìä Experimento: {EXPERIMENT_NAME}\")\n",
        "print(f\"üåê UI disponible en: {MLFLOW_TRACKING_URI}\")\n",
        "\n",
        "# Verificar conexi√≥n\n",
        "try:\n",
        "    client = MlflowClient()\n",
        "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "    print(f\"‚úÖ Experimento encontrado: {experiment.experiment_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è No se pudo conectar a MLflow: {e}\")\n",
        "    print(\"   Aseg√∫rate de tener MLflow server corriendo en http://localhost:5000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preparaci√≥n de Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.data_preparation import prepare_data\n",
        "\n",
        "# Preparar datos\n",
        "print(\"üìä Preparando datos...\")\n",
        "data_dict = prepare_data()\n",
        "\n",
        "X_train = data_dict[\"X_train\"]\n",
        "y_train = data_dict[\"y_train\"]\n",
        "X_val = data_dict[\"X_val\"]\n",
        "y_val = data_dict[\"y_val\"]\n",
        "X_test = data_dict[\"X_test\"]\n",
        "y_test = data_dict[\"y_test\"]\n",
        "\n",
        "print(f\"\\n‚úÖ Datos preparados:\")\n",
        "print(f\"   Train: {X_train.shape}\")\n",
        "print(f\"   Val:   {X_val.shape}\")\n",
        "print(f\"   Test:  {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Entrenamiento de Modelos\n",
        "\n",
        "### 4.1 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üîµ Entrenando Logistic Regression...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Espacio de hiperpar√°metros\n",
        "param_grid_lr = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "base_model_lr = LogisticRegression(random_state=42)\n",
        "\n",
        "# Randomized Search\n",
        "random_search_lr = RandomizedSearchCV(\n",
        "    base_model_lr, param_grid_lr, \n",
        "    n_iter=20, cv=3, scoring='roc_auc',\n",
        "    n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "random_search_lr.fit(X_train, y_train)\n",
        "\n",
        "best_model_lr = random_search_lr.best_estimator_\n",
        "best_params_lr = random_search_lr.best_params_\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lr = best_model_lr.predict(X_val)\n",
        "y_pred_proba_lr = best_model_lr.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "metrics_lr = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred_lr),\n",
        "    'precision': precision_score(y_val, y_pred_lr),\n",
        "    'recall': recall_score(y_val, y_pred_lr),\n",
        "    'f1_score': f1_score(y_val, y_pred_lr),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba_lr),\n",
        "    'log_loss': log_loss(y_val, y_pred_proba_lr)\n",
        "}\n",
        "\n",
        "# Logging en MLflow\n",
        "with mlflow.start_run(run_name=\"LogisticRegression\"):\n",
        "    mlflow.log_params(best_params_lr)\n",
        "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
        "    mlflow.log_metrics(metrics_lr)\n",
        "    mlflow.sklearn.log_model(best_model_lr, \"model\")\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "\n",
        "print(f\"\\n‚úÖ Mejores par√°metros: {best_params_lr}\")\n",
        "print(f\"üìä M√©tricas:\")\n",
        "for metric, value in metrics_lr.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üü¢ Entrenando Random Forest...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Espacio de hiperpar√°metros\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "base_model_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Randomized Search\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    base_model_rf, param_grid_rf,\n",
        "    n_iter=30, cv=3, scoring='roc_auc',\n",
        "    n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "best_model_rf = random_search_rf.best_estimator_\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "\n",
        "# Predicciones\n",
        "y_pred_rf = best_model_rf.predict(X_val)\n",
        "y_pred_proba_rf = best_model_rf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "metrics_rf = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred_rf),\n",
        "    'precision': precision_score(y_val, y_pred_rf),\n",
        "    'recall': recall_score(y_val, y_pred_rf),\n",
        "    'f1_score': f1_score(y_val, y_pred_rf),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba_rf),\n",
        "    'log_loss': log_loss(y_val, y_pred_proba_rf)\n",
        "}\n",
        "\n",
        "# Logging en MLflow\n",
        "with mlflow.start_run(run_name=\"RandomForest\"):\n",
        "    mlflow.log_params(best_params_rf)\n",
        "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
        "    mlflow.log_metrics(metrics_rf)\n",
        "    mlflow.sklearn.log_model(best_model_rf, \"model\")\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "\n",
        "print(f\"\\n‚úÖ Mejores par√°metros: {best_params_rf}\")\n",
        "print(f\"üìä M√©tricas:\")\n",
        "for metric, value in metrics_rf.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üü° Entrenando XGBoost...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Espacio de hiperpar√°metros\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "base_model_xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "\n",
        "# Randomized Search\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    base_model_xgb, param_grid_xgb,\n",
        "    n_iter=30, cv=3, scoring='roc_auc',\n",
        "    n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "best_model_xgb = random_search_xgb.best_estimator_\n",
        "best_params_xgb = random_search_xgb.best_params_\n",
        "\n",
        "# Predicciones\n",
        "y_pred_xgb = best_model_xgb.predict(X_val)\n",
        "y_pred_proba_xgb = best_model_xgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "metrics_xgb = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred_xgb),\n",
        "    'precision': precision_score(y_val, y_pred_xgb),\n",
        "    'recall': recall_score(y_val, y_pred_xgb),\n",
        "    'f1_score': f1_score(y_val, y_pred_xgb),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba_xgb),\n",
        "    'log_loss': log_loss(y_val, y_pred_proba_xgb)\n",
        "}\n",
        "\n",
        "# Logging en MLflow\n",
        "with mlflow.start_run(run_name=\"XGBoost\"):\n",
        "    mlflow.log_params(best_params_xgb)\n",
        "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
        "    mlflow.log_metrics(metrics_xgb)\n",
        "    mlflow.xgboost.log_model(best_model_xgb, \"model\")\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "\n",
        "print(f\"\\n‚úÖ Mejores par√°metros: {best_params_xgb}\")\n",
        "print(f\"üìä M√©tricas:\")\n",
        "for metric, value in metrics_xgb.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 LightGBM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üü£ Entrenando LightGBM...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Espacio de hiperpar√°metros\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9, -1],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'num_leaves': [31, 50, 100, 200],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "base_model_lgb = LGBMClassifier(random_state=42, verbose=-1)\n",
        "\n",
        "# Randomized Search\n",
        "random_search_lgb = RandomizedSearchCV(\n",
        "    base_model_lgb, param_grid_lgb,\n",
        "    n_iter=30, cv=3, scoring='roc_auc',\n",
        "    n_jobs=-1, random_state=42, verbose=1\n",
        ")\n",
        "\n",
        "random_search_lgb.fit(X_train, y_train)\n",
        "\n",
        "best_model_lgb = random_search_lgb.best_estimator_\n",
        "best_params_lgb = random_search_lgb.best_params_\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lgb = best_model_lgb.predict(X_val)\n",
        "y_pred_proba_lgb = best_model_lgb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# M√©tricas\n",
        "metrics_lgb = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred_lgb),\n",
        "    'precision': precision_score(y_val, y_pred_lgb),\n",
        "    'recall': recall_score(y_val, y_pred_lgb),\n",
        "    'f1_score': f1_score(y_val, y_pred_lgb),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba_lgb),\n",
        "    'log_loss': log_loss(y_val, y_pred_proba_lgb)\n",
        "}\n",
        "\n",
        "# Logging en MLflow\n",
        "with mlflow.start_run(run_name=\"LightGBM\"):\n",
        "    mlflow.log_params(best_params_lgb)\n",
        "    mlflow.log_param(\"model_type\", \"LightGBM\")\n",
        "    mlflow.log_metrics(metrics_lgb)\n",
        "    mlflow.lightgbm.log_model(best_model_lgb, \"model\")\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "\n",
        "print(f\"\\n‚úÖ Mejores par√°metros: {best_params_lgb}\")\n",
        "print(f\"üìä M√©tricas:\")\n",
        "for metric, value in metrics_lgb.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparaci√≥n de Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame de comparaci√≥n\n",
        "comparison_data = {\n",
        "    'Logistic Regression': metrics_lr,\n",
        "    'Random Forest': metrics_rf,\n",
        "    'XGBoost': metrics_xgb,\n",
        "    'LightGBM': metrics_lgb\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data).T\n",
        "comparison_df = comparison_df.sort_values('roc_auc', ascending=False)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìä COMPARACI√ìN DE MODELOS (Validation Set)\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    comparison_df[metric].plot(kind='barh', ax=ax, color='steelblue')\n",
        "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "    ax.set_xlabel('Score')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identificar mejor modelo\n",
        "best_model_name = comparison_df.index[0]\n",
        "best_roc_auc = comparison_df.loc[best_model_name, 'roc_auc']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   ROC-AUC: {best_roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluaci√≥n del Mejor Modelo en Test Set\n",
        "\n",
        "Seleccionamos el mejor modelo y lo evaluamos en el conjunto de test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar mejor modelo\n",
        "models_dict = {\n",
        "    'Logistic Regression': best_model_lr,\n",
        "    'Random Forest': best_model_rf,\n",
        "    'XGBoost': best_model_xgb,\n",
        "    'LightGBM': best_model_lgb\n",
        "}\n",
        "\n",
        "best_model = models_dict[best_model_name]\n",
        "\n",
        "# Predicciones en test\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "y_pred_proba_test = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# M√©tricas en test\n",
        "metrics_test = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_test),\n",
        "    'precision': precision_score(y_test, y_pred_test),\n",
        "    'recall': recall_score(y_test, y_pred_test),\n",
        "    'f1_score': f1_score(y_test, y_pred_test),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_test),\n",
        "    'log_loss': log_loss(y_test, y_pred_proba_test)\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä EVALUACI√ìN DEL MEJOR MODELO ({best_model_name}) EN TEST SET\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nM√©tricas:\")\n",
        "for metric, value in metrics_test.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Classification Report:\")\n",
        "print(\"-\"*70)\n",
        "print(classification_report(y_test, y_pred_test, \n",
        "                            target_names=['No Abandono', 'Abandono']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizaciones de Evaluaci√≥n\n",
        "\n",
        "### 7.1 Matriz de Confusi√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Abandono', 'Abandono'],\n",
        "            yticklabels=['No Abandono', 'Abandono'])\n",
        "plt.title(f'Matriz de Confusi√≥n - {best_model_name}')\n",
        "plt.ylabel('Verdadero')\n",
        "plt.xlabel('Predicho')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Curva ROC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'ROC curve (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
        "         label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title(f'Curva ROC - {best_model_name}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Curva Precision-Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_test)\n",
        "avg_precision = np.trapz(precision, recall)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2,\n",
        "         label=f'Precision-Recall (AP = {avg_precision:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Curva Precision-Recall - {best_model_name}')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Feature Importance (si est√° disponible)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intentar obtener feature importance\n",
        "try:\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        importances = best_model.feature_importances_\n",
        "    elif hasattr(best_model, 'coef_'):\n",
        "        importances = np.abs(best_model.coef_[0])\n",
        "    else:\n",
        "        importances = None\n",
        "    \n",
        "    if importances is not None:\n",
        "        # Obtener nombres de features (si est√°n disponibles)\n",
        "        feature_names = data_dict.get('feature_names', [f'Feature_{i}' for i in range(len(importances))])\n",
        "        \n",
        "        # Crear DataFrame\n",
        "        feature_imp_df = pd.DataFrame({\n",
        "            'feature': feature_names[:len(importances)],\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False).head(20)\n",
        "        \n",
        "        # Visualizar\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.barh(range(len(feature_imp_df)), feature_imp_df['importance'])\n",
        "        plt.yticks(range(len(feature_imp_df)), feature_imp_df['feature'])\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.title(f'Top 20 Features m√°s Importantes - {best_model_name}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\\nTop 10 Features m√°s Importantes:\")\n",
        "        print(feature_imp_df.head(10).to_string(index=False))\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Feature importance no disponible para este tipo de modelo\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error al calcular feature importance: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparaci√≥n de Curvas ROC de Todos los Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular curvas ROC para todos los modelos en test\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Obtener predicciones de todos los modelos en test\n",
        "models_predictions = {\n",
        "    'Logistic Regression': best_model_lr.predict_proba(X_test)[:, 1],\n",
        "    'Random Forest': best_model_rf.predict_proba(X_test)[:, 1],\n",
        "    'XGBoost': best_model_xgb.predict_proba(X_test)[:, 1],\n",
        "    'LightGBM': best_model_lgb.predict_proba(X_test)[:, 1]\n",
        "}\n",
        "\n",
        "for model_name, y_proba in models_predictions.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {auc:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
        "         label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Comparaci√≥n de Curvas ROC - Todos los Modelos (Test Set)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Resumen Final y Conclusiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìä RESUMEN FINAL DEL MODELADO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. MODELOS ENTRENADOS:\")\n",
        "print(\"   ‚úÖ Logistic Regression\")\n",
        "print(\"   ‚úÖ Random Forest\")\n",
        "print(\"   ‚úÖ XGBoost\")\n",
        "print(\"   ‚úÖ LightGBM\")\n",
        "\n",
        "print(f\"\\n2. MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   ROC-AUC (Validation): {comparison_df.loc[best_model_name, 'roc_auc']:.4f}\")\n",
        "print(f\"   ROC-AUC (Test):       {metrics_test['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n3. M√âTRICAS EN TEST SET:\")\n",
        "for metric, value in metrics_test.items():\n",
        "    print(f\"   {metric:15s}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n4. MLFLOW:\")\n",
        "print(f\"   ‚úÖ Todos los modelos registrados en MLflow\")\n",
        "print(f\"   ‚úÖ Experimento: {EXPERIMENT_NAME}\")\n",
        "print(f\"   üåê UI: {MLFLOW_TRACKING_URI}\")\n",
        "\n",
        "print(\"\\n5. PR√ìXIMOS PASOS:\")\n",
        "print(\"   ‚Ä¢ Revisar resultados en MLflow UI\")\n",
        "print(\"   ‚Ä¢ Registrar el mejor modelo en Model Registry\")\n",
        "print(\"   ‚Ä¢ Preparar presentaci√≥n con screenshots de MLflow\")\n",
        "print(\"   ‚Ä¢ Documentar insights y conclusiones\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
